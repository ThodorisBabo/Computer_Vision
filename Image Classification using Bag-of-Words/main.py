# -*- coding: utf-8 -*-
"""ImageBoWClassifier .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nuOHDtDFjkXVY0RY56wXD0JRiE3wly2f
"""

import os
import cv2
import numpy as np
import pickle
import random
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from tqdm import tqdm

# ========================== DATASET INFORMATION ==========================

def get_random_image(folder_path):
    """Returns a random image path from the specified folder."""
    images = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]
    return os.path.join(folder_path, random.choice(images)) if images else None

def count_images(folder_path):
    """Counts the number of images in a given folder."""
    return len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])

def display_dataset_info(base_folder):
    """Displays dataset information and sample images."""
    fig, axes = plt.subplots(2, 5, figsize=(15, 6))
    fig.suptitle("Random Images from Each Class (Training & Testing)", fontsize=16)

    for i, dataset in enumerate(['imagedb', 'imagedb_test']):
        dataset_path = os.path.join(base_folder, dataset)
        if not os.path.exists(dataset_path):
            print(f"Folder '{dataset}' not found in '{base_folder}'.")
            return

        print(f"\nClass counts in {dataset}:")
        classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]
        for j, class_name in enumerate(classes[:5]):
            class_path = os.path.join(dataset_path, class_name)
            num_images = count_images(class_path)
            print(f"  - {class_name}: {num_images} images")

            image_path = get_random_image(class_path)
            if image_path:
                img = mpimg.imread(image_path)
                axes[i, j].imshow(img)
                axes[i, j].axis('off')
                axes[i, j].set_title(f"{dataset.capitalize()} - {class_name}")
            else:
                axes[i, j].set_visible(False)

    plt.tight_layout()
    plt.show()

# ========================== FEATURE EXTRACTION (SIFT) ==========================

def extract_sift_features(image_path):
    """Extracts SIFT descriptors from an image."""
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if image is None:
        return None

    sift = cv2.SIFT_create()
    _, descriptors = sift.detectAndCompute(image, None)
    return descriptors

def collect_all_descriptors(base_folder):
    """Extracts SIFT descriptors from all training images."""
    all_descriptors = []
    dataset_path = os.path.join(base_folder, "imagedb")

    print("\nExtracting features from images...")
    for class_name in tqdm(os.listdir(dataset_path)):
        class_path = os.path.join(dataset_path, class_name)
        if not os.path.isdir(class_path):
            continue

        for image_file in os.listdir(class_path):
            image_path = os.path.join(class_path, image_file)
            descriptors = extract_sift_features(image_path)
            if descriptors is not None:
                all_descriptors.extend(descriptors)

    return np.array(all_descriptors) if all_descriptors else None

# ========================== CLUSTERING: CREATING VISUAL WORDS ==========================

def create_vocabulary(descriptors, num_clusters=50):
    """Clusters descriptors using KMeans to form a visual vocabulary."""
    print("\nClustering descriptors to form vocabulary...")
    kmeans = KMeans(n_clusters=num_clusters, random_state=41, n_init=10)
    kmeans.fit(descriptors)
    return kmeans

# ========================== BAG-OF-WORDS REPRESENTATION ==========================

def compute_histogram(image_path, kmeans):
    """Computes the BoW histogram representation for an image."""
    descriptors = extract_sift_features(image_path)
    if descriptors is None:
        return np.zeros(kmeans.n_clusters)

    labels = kmeans.predict(descriptors)
    histogram = np.bincount(labels, minlength=kmeans.n_clusters)
    return histogram

def create_bow_representations(base_folder, kmeans):
    """Generates BoW feature vectors for training images."""
    dataset_path = os.path.join(base_folder, "imagedb")
    bow_features, image_labels = [], []

    print("\nCreating BoW representations...")
    for class_name in tqdm(os.listdir(dataset_path)):
        class_path = os.path.join(dataset_path, class_name)
        if not os.path.isdir(class_path):
            continue

        for image_file in os.listdir(class_path):
            image_path = os.path.join(class_path, image_file)
            histogram = compute_histogram(image_path, kmeans)
            bow_features.append(histogram)
            image_labels.append(class_name)

    return np.array(bow_features), np.array(image_labels)

# ========================== SAVE & LOAD FUNCTIONS ==========================

def save_bow_data(kmeans_model, bow_features, image_labels):
    """Saves the trained KMeans model and BoW features."""
    with open("bow_model.pkl", "wb") as f:
        pickle.dump(kmeans_model, f)
    np.save("bow_features.npy", bow_features)
    np.save("image_labels.npy", image_labels)
    print("\nBoW model and features saved successfully!")

def load_bow_data():
    """Loads the saved BoW features and labels."""
    with open("bow_model.pkl", "rb") as f:
        kmeans_model = pickle.load(f)
    bow_features = np.load("bow_features.npy")
    image_labels = np.load("image_labels.npy")
    return kmeans_model, bow_features, image_labels

# ========================== KNN CLASSIFICATION ==========================

def extract_test_bow_features(base_folder, kmeans):
    """Extracts BoW feature vectors for test images."""
    dataset_path = os.path.join(base_folder, "imagedb_test")
    test_features, test_labels = [], []

    print("\nExtracting BoW features for test images...")
    for class_name in tqdm(os.listdir(dataset_path)):
        class_path = os.path.join(dataset_path, class_name)
        if not os.path.isdir(class_path):
            continue

        for image_file in os.listdir(class_path):
            image_path = os.path.join(class_path, image_file)
            histogram = compute_histogram(image_path, kmeans)
            test_features.append(histogram)
            test_labels.append(class_name)

    return np.array(test_features), np.array(test_labels)

def train_and_evaluate_knn(train_features, train_labels, test_features, test_labels, k=5):
    """Trains KNN on BoW features and evaluates it."""
    print("\nTraining KNN model...")
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(train_features, train_labels)

    print("\nEvaluating KNN model...")
    train_accuracy = accuracy_score(train_labels, knn.predict(train_features))
    test_accuracy = accuracy_score(test_labels, knn.predict(test_features))

    print(f"\nTraining Accuracy: {train_accuracy:.2%}")
    print(f"Test Accuracy: {test_accuracy:.2%}")

    print("\nClassification Report:")
    print(classification_report(test_labels, knn.predict(test_features)))


# ========================== SVM CLASSIFICATION ==========================

def train_and_evaluate_svm(train_features, train_labels, test_features, test_labels):
    """Trains an SVM model on BoW features and evaluates it."""
    print("\nTraining SVM model...")
    svm = SVC(kernel='linear', C=1.0)  # Linear kernel for multi-class classification
    svm.fit(train_features, train_labels)

    print("\nEvaluating SVM model...")
    train_accuracy = accuracy_score(train_labels, svm.predict(train_features))
    test_accuracy = accuracy_score(test_labels, svm.predict(test_features))

    print(f"\nSVM Training Accuracy: {train_accuracy:.2%}")
    print(f"SVM Test Accuracy: {test_accuracy:.2%}")

    print("\nSVM Classification Report:")
    print(classification_report(test_labels, svm.predict(test_features)))

# ========================== MAIN FUNCTION ==========================

def main():
    base_folder = "/content/drive/MyDrive/Dataset/caltech"

    # Step 1: Display dataset info
    display_dataset_info(base_folder)

    # Step 2: Extract SIFT descriptors
    descriptors = collect_all_descriptors(base_folder)

    # Step 3: Create vocabulary and BoW representations
    kmeans_model = create_vocabulary(descriptors, num_clusters=50)
    bow_features, image_labels = create_bow_representations(base_folder, kmeans_model)

    # Save BoW data
    save_bow_data(kmeans_model, bow_features, image_labels)

    # Load BoW data
    kmeans_model, train_features, train_labels = load_bow_data()

    # Step 4: Create BoW representations for Test Images
    test_features, test_labels = extract_test_bow_features(base_folder, kmeans_model)

    # Step 5: Evaluate the models
    train_and_evaluate_knn(train_features, train_labels, test_features, test_labels, k=5)

    # Step 6: Evaluate SVM model
    train_and_evaluate_svm(train_features, train_labels, test_features, test_labels)

# Run the program
if __name__ == "__main__":
    main()
